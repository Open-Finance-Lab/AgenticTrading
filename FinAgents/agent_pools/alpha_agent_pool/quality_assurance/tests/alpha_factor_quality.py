"""
Alpha Factor Quality Testing Framework

This module implements comprehensive statistical and financial tests to validate
the quality of alpha factors generated by agents. Tests focus on factor significance,
stability, and predictive power for cross-sectional returns.

Key Testing Areas:
1. Statistical Significance Tests
2. Factor Stability and Decay Analysis
3. Cross-Sectional Information Coefficient (IC) Analysis
4. Factor Correlation and Orthogonality Tests
5. Turnover and Implementation Cost Analysis

Author: FinAgent Quality Assurance Team
"""

import numpy as np
import pandas as pd
import scipy.stats as stats
from typing import Dict, List, Tuple, Optional, Union
from dataclasses import dataclass
from datetime import datetime, timedelta
import logging
import warnings
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns

logger = logging.getLogger(__name__)

@dataclass
class FactorQualityMetrics:
    """Container for factor quality assessment metrics"""
    ic_mean: float
    ic_std: float
    ic_ir: float  # Information Ratio
    t_stat: float
    p_value: float
    hit_rate: float
    factor_decay: Dict[int, float]
    turnover: float
    max_drawdown: float
    stability_score: float

class AlphaFactorQualityTests:
    """
    Comprehensive alpha factor quality testing suite for agent-generated factors.
    
    This class provides statistical and financial validation of alpha factors,
    ensuring they meet academic and industry standards for quantitative trading.
    """
    
    def __init__(self, significance_level: float = 0.05):
        """
        Initialize factor quality testing framework.
        
        Args:
            significance_level: Statistical significance threshold for tests
        """
        self.significance_level = significance_level
        self.scaler = StandardScaler()
        
    def compute_information_coefficient(
        self, 
        factor_values: pd.DataFrame, 
        forward_returns: pd.DataFrame,
        method: str = 'spearman'
    ) -> pd.Series:
        """
        Compute Information Coefficient between factors and forward returns.
        
        Args:
            factor_values: DataFrame with factor values (dates x symbols)
            forward_returns: DataFrame with forward returns (dates x symbols)
            method: Correlation method ('spearman', 'pearson')
            
        Returns:
            Series of IC values indexed by date
        """
        ic_series = []
        
        for date in factor_values.index:
            if date in forward_returns.index:
                factor_cross_section = factor_values.loc[date].dropna()
                return_cross_section = forward_returns.loc[date].dropna()
                
                # Find common symbols
                common_symbols = factor_cross_section.index.intersection(
                    return_cross_section.index
                )
                
                if len(common_symbols) >= 10:  # Minimum cross-section size
                    factor_vals = factor_cross_section[common_symbols]
                    return_vals = return_cross_section[common_symbols]
                    
                    # Ensure both arrays have the same length and no NaN values
                    factor_vals = factor_vals.dropna()
                    return_vals = return_vals.dropna()
                    
                    # Re-align to common index after dropping NaN
                    common_valid = factor_vals.index.intersection(return_vals.index)
                    
                    if len(common_valid) >= 10:
                        factor_vals = factor_vals[common_valid]
                        return_vals = return_vals[common_valid]
                        
                        try:
                            if method == 'spearman':
                                ic, _ = stats.spearmanr(factor_vals, return_vals)
                            else:
                                ic, _ = stats.pearsonr(factor_vals, return_vals)
                            
                            ic_series.append((date, ic if not np.isnan(ic) else 0.0))
                        except Exception as e:
                            logger.debug(f"IC calculation failed for date {date}: {e}")
                            ic_series.append((date, 0.0))
        
        ic_df = pd.DataFrame(ic_series, columns=['date', 'ic'])
        return ic_df.set_index('date')['ic']
    
    def test_factor_significance(
        self, 
        factor_values: pd.DataFrame, 
        forward_returns: pd.DataFrame
    ) -> Dict[str, float]:
        """
        Test statistical significance of factor predictive power.
        
        Args:
            factor_values: Factor values over time
            forward_returns: Forward returns for prediction target
            
        Returns:
            Dictionary containing significance test results
        """
        ic_series = self.compute_information_coefficient(factor_values, forward_returns)
        
        # Remove NaN values
        ic_clean = ic_series.dropna()
        
        if len(ic_clean) < 10:
            logger.warning("Insufficient data points for significance testing")
            return {
                'ic_mean': 0.0,
                'ic_std': 0.0,
                'ic_ir': 0.0,
                't_stat': 0.0,
                'p_value': 1.0,
                'is_significant': False
            }
        
        ic_mean = ic_clean.mean()
        ic_std = ic_clean.std()
        ic_ir = ic_mean / ic_std if ic_std > 0 else 0.0
        
        # T-test for mean IC significantly different from zero
        t_stat, p_value = stats.ttest_1samp(ic_clean, 0.0)
        
        is_significant = p_value < self.significance_level
        
        return {
            'ic_mean': ic_mean,
            'ic_std': ic_std,
            'ic_ir': ic_ir,
            't_stat': t_stat,
            'p_value': p_value,
            'is_significant': is_significant
        }
    
    def analyze_factor_decay(
        self, 
        factor_values: pd.DataFrame, 
        returns_data: pd.DataFrame,
        max_horizon: int = 20
    ) -> Dict[int, float]:
        """
        Analyze factor decay over different forward-looking horizons.
        
        Args:
            factor_values: Factor values over time
            returns_data: Daily returns data
            max_horizon: Maximum days to analyze decay
            
        Returns:
            Dictionary mapping horizon days to IC values
        """
        decay_results = {}
        
        for horizon in range(1, max_horizon + 1):
            # Compute forward returns for this horizon
            forward_returns = returns_data.shift(-horizon).rolling(horizon).sum()
            
            # Compute IC for this horizon
            ic_series = self.compute_information_coefficient(
                factor_values, forward_returns
            )
            
            decay_results[horizon] = ic_series.mean() if not ic_series.empty else 0.0
        
        return decay_results
    
    def compute_hit_rate(
        self, 
        factor_values: pd.DataFrame, 
        forward_returns: pd.DataFrame,
        quantiles: int = 5
    ) -> float:
        """
        Compute hit rate by analyzing top/bottom quantile performance.
        
        Args:
            factor_values: Factor values over time
            forward_returns: Forward returns data
            quantiles: Number of quantiles for ranking
            
        Returns:
            Hit rate (fraction of times top quantile outperforms bottom)
        """
        hit_count = 0
        total_count = 0
        
        for date in factor_values.index:
            if date in forward_returns.index:
                factor_cross_section = factor_values.loc[date].dropna()
                return_cross_section = forward_returns.loc[date].dropna()
                
                common_symbols = factor_cross_section.index.intersection(
                    return_cross_section.index
                )
                
                if len(common_symbols) >= quantiles * 2:
                    factor_vals = factor_cross_section[common_symbols]
                    return_vals = return_cross_section[common_symbols]
                    
                    # Create quantile rankings
                    factor_quantiles = pd.qcut(
                        factor_vals, 
                        quantiles, 
                        labels=False, 
                        duplicates='drop'
                    )
                    
                    # Compare top vs bottom quantile returns
                    top_quantile_mask = factor_quantiles == (quantiles - 1)
                    bottom_quantile_mask = factor_quantiles == 0
                    
                    if top_quantile_mask.sum() > 0 and bottom_quantile_mask.sum() > 0:
                        top_return = return_vals[top_quantile_mask].mean()
                        bottom_return = return_vals[bottom_quantile_mask].mean()
                        
                        if top_return > bottom_return:
                            hit_count += 1
                        total_count += 1
        
        return hit_count / total_count if total_count > 0 else 0.5
    
    def compute_factor_turnover(
        self, 
        factor_values: pd.DataFrame,
        quantiles: int = 5
    ) -> float:
        """
        Compute factor turnover to assess implementation costs.
        
        Args:
            factor_values: Factor values over time
            quantiles: Number of quantiles for ranking
            
        Returns:
            Average turnover rate
        """
        turnover_rates = []
        
        for i in range(1, len(factor_values)):
            prev_date = factor_values.index[i-1]
            curr_date = factor_values.index[i]
            
            prev_factors = factor_values.loc[prev_date].dropna()
            curr_factors = factor_values.loc[curr_date].dropna()
            
            common_symbols = prev_factors.index.intersection(curr_factors.index)
            
            if len(common_symbols) >= quantiles * 2:
                prev_quantiles = pd.qcut(
                    prev_factors[common_symbols], 
                    quantiles, 
                    labels=False,
                    duplicates='drop'
                )
                curr_quantiles = pd.qcut(
                    curr_factors[common_symbols], 
                    quantiles, 
                    labels=False,
                    duplicates='drop'
                )
                
                # Calculate turnover as fraction of positions that changed
                changed_positions = (prev_quantiles != curr_quantiles).sum()
                turnover_rate = changed_positions / len(common_symbols)
                turnover_rates.append(turnover_rate)
        
        return np.mean(turnover_rates) if turnover_rates else 0.0
    
    def assess_factor_stability(
        self, 
        factor_values: pd.DataFrame,
        window_size: int = 60
    ) -> float:
        """
        Assess factor stability using rolling window analysis.
        
        Args:
            factor_values: Factor values over time
            window_size: Rolling window size for stability analysis
            
        Returns:
            Stability score (higher is more stable)
        """
        if len(factor_values) < window_size * 2:
            return 0.0
        
        correlation_scores = []
        
        for i in range(window_size, len(factor_values) - window_size):
            window1 = factor_values.iloc[i-window_size:i]
            window2 = factor_values.iloc[i:i+window_size]
            
            # Calculate correlation between overlapping symbols
            corr_scores = []
            for symbol in window1.columns:
                if symbol in window2.columns:
                    vals1 = window1[symbol].dropna()
                    vals2 = window2[symbol].dropna()
                    
                    # Ensure data alignment by common index
                    common_idx = vals1.index.intersection(vals2.index)
                    
                    if len(common_idx) > 10:
                        vals1_aligned = vals1[common_idx]
                        vals2_aligned = vals2[common_idx]
                        
                        try:
                            corr, _ = stats.pearsonr(vals1_aligned, vals2_aligned)
                            if not np.isnan(corr):
                                corr_scores.append(abs(corr))
                        except Exception as e:
                            logger.debug(f"Correlation calculation failed for {symbol}: {e}")
            
            if corr_scores:
                correlation_scores.append(np.mean(corr_scores))
        
        return np.mean(correlation_scores) if correlation_scores else 0.0
    
    def run_comprehensive_quality_assessment(
        self, 
        factor_values: pd.DataFrame, 
        returns_data: pd.DataFrame,
        forward_horizon: int = 1
    ) -> FactorQualityMetrics:
        """
        Run comprehensive quality assessment on alpha factor.
        
        Args:
            factor_values: Factor values over time
            returns_data: Returns data for validation
            forward_horizon: Forward return horizon for prediction
            
        Returns:
            FactorQualityMetrics object with all quality metrics
        """
        logger.info("Running comprehensive alpha factor quality assessment")
        
        # Compute forward returns
        forward_returns = returns_data.shift(-forward_horizon)
        
        # Statistical significance tests
        significance_results = self.test_factor_significance(
            factor_values, forward_returns
        )
        
        # Factor decay analysis
        decay_results = self.analyze_factor_decay(
            factor_values, returns_data
        )
        
        # Hit rate analysis
        hit_rate = self.compute_hit_rate(factor_values, forward_returns)
        
        # Turnover analysis
        turnover = self.compute_factor_turnover(factor_values)
        
        # Stability assessment
        stability_score = self.assess_factor_stability(factor_values)
        
        # Compute maximum drawdown of IC series
        ic_series = self.compute_information_coefficient(factor_values, forward_returns)
        ic_cumsum = ic_series.cumsum()
        running_max = ic_cumsum.expanding().max()
        drawdown = ic_cumsum - running_max
        max_drawdown = drawdown.min()
        
        return FactorQualityMetrics(
            ic_mean=significance_results['ic_mean'],
            ic_std=significance_results['ic_std'],
            ic_ir=significance_results['ic_ir'],
            t_stat=significance_results['t_stat'],
            p_value=significance_results['p_value'],
            hit_rate=hit_rate,
            factor_decay=decay_results,
            turnover=turnover,
            max_drawdown=max_drawdown,
            stability_score=stability_score
        )
    
    def generate_quality_report(
        self, 
        metrics: FactorQualityMetrics,
        factor_name: str = "Alpha Factor"
    ) -> str:
        """
        Generate comprehensive quality assessment report.
        
        Args:
            metrics: Factor quality metrics
            factor_name: Name of the factor being assessed
            
        Returns:
            Formatted quality report string
        """
        report = f"""
Alpha Factor Quality Assessment Report
=====================================

Factor Name: {factor_name}
Assessment Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

STATISTICAL SIGNIFICANCE
------------------------
Information Coefficient (Mean): {metrics.ic_mean:.4f}
Information Coefficient (Std):  {metrics.ic_std:.4f}
Information Ratio:              {metrics.ic_ir:.4f}
T-Statistic:                    {metrics.t_stat:.4f}
P-Value:                        {metrics.p_value:.4f}
Statistical Significance:       {'✓ PASS' if metrics.p_value < self.significance_level else '✗ FAIL'}

PREDICTIVE POWER
----------------
Hit Rate:                       {metrics.hit_rate:.2%}
Hit Rate Assessment:            {'✓ PASS' if metrics.hit_rate > 0.55 else '✗ FAIL'}

IMPLEMENTATION FEASIBILITY
--------------------------
Factor Turnover:                {metrics.turnover:.2%}
Turnover Assessment:            {'✓ PASS' if metrics.turnover < 0.5 else '✗ FAIL'}
Max IC Drawdown:                {metrics.max_drawdown:.4f}

FACTOR STABILITY
----------------
Stability Score:                {metrics.stability_score:.4f}
Stability Assessment:           {'✓ PASS' if metrics.stability_score > 0.3 else '✗ FAIL'}

FACTOR DECAY ANALYSIS
---------------------"""
        
        for horizon, ic_value in list(metrics.factor_decay.items())[:10]:
            report += f"\nHorizon {horizon:2d} days: IC = {ic_value:.4f}"
        
        # Overall assessment
        pass_count = sum([
            metrics.p_value < self.significance_level,
            metrics.hit_rate > 0.55,
            metrics.turnover < 0.5,
            metrics.stability_score > 0.3
        ])
        
        report += f"""

OVERALL ASSESSMENT
------------------
Tests Passed: {pass_count}/4
Overall Quality: {'✓ HIGH QUALITY' if pass_count >= 3 else '⚠ NEEDS IMPROVEMENT' if pass_count >= 2 else '✗ LOW QUALITY'}

RECOMMENDATIONS
---------------"""
        
        if metrics.p_value >= self.significance_level:
            report += "\n• Factor lacks statistical significance - consider feature engineering"
        if metrics.hit_rate <= 0.55:
            report += "\n• Low hit rate - review factor construction methodology"
        if metrics.turnover >= 0.5:
            report += "\n• High turnover - consider smoothing or longer holding periods"
        if metrics.stability_score <= 0.3:
            report += "\n• Factor instability detected - implement regime-aware adjustments"
        
        if pass_count >= 3:
            report += "\n• Factor meets quality standards for production deployment"
        
        return report
