"""
Alpha Research Agent - åŸºäº OpenAI Agents SDK æ„å»º (ä¿®æ­£ç‰ˆ)
æ”¯æŒæ–°ç‰ˆ openai>=1.0 API å’Œè‡ªåŠ¨ context ä¼ é€’
"""

import os
import asyncio
import json
from datetime import datetime
from typing import Dict, List, Any, Optional
import pandas as pd
from pydantic import BaseModel
import numpy as np
import qlib
from qlib.data import D
from qlib.contrib.data.handler import Alpha158

# âœ… å¼•å…¥ä¿®å¤åçš„ Agent
from agents import Agent, function_tool

# ä½ çš„è‡ªå®šä¹‰å·¥å…·æ¨¡å—ï¼ˆè¯·ç¡®ä¿è·¯å¾„æ­£ç¡®ï¼‰
from alpha_analysis_toolkit import AlphaAnalysisToolkit
from alpha_visualization_toolkit import AlphaVisualizationToolkit


# ============================================================================
# æ•°æ®ç»“æ„å®šä¹‰
# ============================================================================

class FactorPerformance(BaseModel):
    """å› å­é¢„æœŸè¡¨ç°"""
    market_regime: str
    confidence_level: float
    expected_sharpe: float

    class Config:
        extra = "forbid"


class FactorProposal(BaseModel):
    """å•ä¸ªå› å­å»ºè®®"""
    factor_name: str
    description: str
    formula: str
    justification: str
    expected_performance: FactorPerformance

    class Config:
        extra = "forbid"


class AlphaFactorResponse(BaseModel):
    """LLMç»“æ„åŒ–å› å­å»ºè®®å“åº”"""
    factor_proposals: List[FactorProposal]
    market_summary: str
    risk_assessment: str

    class Config:
        extra = "forbid"


# ============================================================================
# Contextç±»ï¼šå­˜å‚¨æ‰§è¡ŒçŠ¶æ€
# ============================================================================

class AlphaResearchContext:
    """Alphaç ”ç©¶ä¸Šä¸‹æ–‡"""
    def __init__(self):
        self.current_asset: Optional[str] = None
        self.market_data: Optional[pd.DataFrame] = None
        self.analysis_results: Dict[str, Any] = {}
        self.visualizations: Dict[str, Any] = {}
        self.factor_proposals: List[Dict] = []
        self.iteration_count: int = 0
        self.execution_log: List[Dict[str, Any]] = []
        self.session_id: str = datetime.now().strftime("%Y%m%d_%H%M%S")

    def log_function_call(self, name: str, args: Dict, result: str, execution_time: float = 0):
        self.execution_log.append({
            'timestamp': datetime.now().isoformat(),
            'function_name': name,
            'arguments': args,
            'result_preview': result[:500],
            'result_length': len(result),
            'execution_time': execution_time,
            'step_number': len(self.execution_log) + 1
        })

    def save_session_log(self, output_dir="agent_logs"):
        os.makedirs(output_dir, exist_ok=True)
        log_path = os.path.join(output_dir, f"session_{self.session_id}.json")
        with open(log_path, "w", encoding="utf-8") as f:
            json.dump({
                "session_id": self.session_id,
                "execution_log": self.execution_log,
                "summary": {
                    "steps": len(self.execution_log),
                    "asset": self.current_asset,
                    "data_loaded": self.market_data is not None,
                    "factor_count": len(self.factor_proposals)
                }
            }, f, ensure_ascii=False, indent=2)
        return log_path


# ============================================================================
# å·¥å…·å‡½æ•°ï¼ˆè£…é¥°ä¸º function_toolï¼‰
# ============================================================================

@function_tool
def load_and_analyze_data(ctx: AlphaResearchContext, csv_path: str, qlib_format: bool = False):
    """åŠ è½½å¹¶åˆ†æèµ„äº§æ•°æ®ï¼Œè®¡ç®—æŠ€æœ¯æŒ‡æ ‡å’Œä¿¡å·"""
    start = datetime.now()
    try:
        print(f"ğŸ”§ åŠ è½½æ•°æ®: {csv_path}")
        data = AlphaAnalysisToolkit.load_asset_data(csv_path, data_format="csv")
        data = AlphaAnalysisToolkit.preprocess_data(data, qlib_format=qlib_format)
        indicators = AlphaAnalysisToolkit.calculate_technical_indicators(data)
        signals = AlphaAnalysisToolkit.generate_alpha_signals(data)
        risk = AlphaAnalysisToolkit.calculate_risk_metrics(data)

        ctx.current_asset = csv_path
        ctx.market_data = data
        ctx.analysis_results = {
            "technical_indicators": indicators,
            "signals": signals,
            "risk_metrics": risk
        }

        summary = (
            f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {csv_path}\n"
            f"ğŸ“Š è¡Œæ•°={len(data)} | æŒ‡æ ‡={len(indicators)} | ä¿¡å·={len(signals)}\n"
            f"âš ï¸ å¹´åŒ–æ³¢åŠ¨ç‡={risk.get('volatility', 0):.2%} | Sharpe={risk.get('sharpe_ratio', 0):.2f}"
        )

        ctx.log_function_call("load_and_analyze_data", {"csv_path": csv_path}, summary,
                              (datetime.now() - start).total_seconds())
        return summary
    except Exception as e:
        return f"âŒ åŠ è½½å¤±è´¥: {e}"


@function_tool
def load_qlib_factors(ctx: AlphaResearchContext,
                      top_n: int = 30,
                      start_date: str = "2022-08-16",
                      end_date: str = "2024-12-31"):
    """
    ç”¨ Alpha158 Handler è®¡ç®—å¹¶è¯„ä¼° IC/IRï¼ˆæ­£ç¡®æ–¹å¼ï¼‰
    """
    from qlib.data.dataset import DatasetH
    from qlib.contrib.data.handler import Alpha158

    start_time = datetime.now()
    try:
        provider_uri = "/content/AgenticTradng/qlib_data/stock_custom_day"
        feats_dir = os.path.join(provider_uri, "features")
        inst_list = sorted([d for d in os.listdir(feats_dir)
                            if os.path.isdir(os.path.join(feats_dir, d))])
        print(f"æ£€æµ‹åˆ° {len(inst_list)} æ”¯è‚¡ç¥¨: {inst_list[:5]} ...")

        # === 1) æ„å»º DatasetH ===
        handler_cfg = {
            "class": "Alpha158",
            "module_path": "qlib.contrib.data.handler",
            "kwargs": {
                "start_time": start_date,
                "end_time": end_date,
                "fit_start_time": start_date,
                "fit_end_time": end_date,
                "instruments": inst_list,
            },
        }
        ds = DatasetH(handler=handler_cfg, segments={"all": (start_date, end_date)})

        feat_df = ds.prepare("all", col_set="feature")
        valid_cols = [c for c in feat_df.columns
                      if feat_df[c].dropna().std() > 0 and feat_df[c].notna().sum() > 50]
        feat_df = feat_df[valid_cols]
        print(f"æœ‰æ•ˆå› å­æ•°: {len(valid_cols)}")

        # === 2) æœªæ¥ä¸€å¤©æ”¶ç›Š ===
        close = (D.features(instruments=inst_list, fields=["$close"],
                            start_time=start_date, end_time=end_date, freq="day")
                 .reset_index().set_index(["datetime", "instrument"]))
        ret_fwd = close["$close"].groupby(level=1).pct_change().shift(-1).rename("ret_fwd")

        panel = feat_df.join(ret_fwd, how="inner").replace([np.inf, -np.inf], np.nan).dropna(subset=["ret_fwd"])
        gb = panel.groupby(level=0)

        # === 3) è®¡ç®— IC/IR ===
        records = []
        for f in valid_cols:
            ic_by_day = gb.apply(lambda g: g[f].corr(g["ret_fwd"], method="spearman")).dropna()
            if len(ic_by_day) < 10:
                continue
            ic_mean, ic_std = ic_by_day.mean(), ic_by_day.std(ddof=1)
            ir = ic_mean / ic_std if ic_std > 0 else np.nan
            records.append({"factor": f, "IC": float(ic_mean), "IR": float(ir), "days": len(ic_by_day)})

        perf_df = pd.DataFrame(records).dropna().sort_values("IR", ascending=False)
        ctx.analysis_results["qlib_factors"] = perf_df

        summary = f"""
âœ… æˆåŠŸè®¡ç®— {len(perf_df)} ä¸ª Alpha158 å› å­è¡¨ç°
å‰5åï¼ˆæŒ‰IRæ’åºï¼‰:
{perf_df.head(5).to_string(index=False)}
"""
        ctx.log_function_call("load_qlib_factors",
                              {"provider": provider_uri, "top_n": top_n},
                              summary, (datetime.now() - start_time).total_seconds())
        print(summary)
        return summary

    except Exception as e:
        error_msg = f"âŒ åŠ è½½ Alpha158 å› å­å¤±è´¥: {e}"
        ctx.log_function_call("load_qlib_factors", {}, error_msg, 0)
        print(error_msg)
        return error_msg



@function_tool
def propose_alpha_factors(ctx: AlphaResearchContext):
    """æ ¹æ®Qlibç»“æœæå‡ºAlphaå› å­å»ºè®®"""
    perf_df = ctx.analysis_results.get("qlib_factors")
    if perf_df is None or perf_df.empty:
        return "âš ï¸ è¯·å…ˆè¿è¡Œ load_qlib_factors()"
    ctx.factor_proposals = perf_df.head(5).to_dict("records")
    result = "\n".join(
        [f"{i+1}. {r['factor']} | IC={r['IC']:.3f} | IR={r['IR']:.2f}"
         for i, r in enumerate(ctx.factor_proposals)]
    )
    ctx.log_function_call("propose_alpha_factors", {}, result, 0)
    return result


@function_tool
def generate_iteration_report(ctx: AlphaResearchContext, iteration_number: int = 1):
    """ç”Ÿæˆè¿­ä»£æŠ¥å‘Š"""
    ctx.iteration_count = iteration_number
    report = f"""
ğŸ“‹ ç ”ç©¶æŠ¥å‘Š #{iteration_number}
èµ„äº§: {ctx.current_asset}
åˆ†æç»“æœ: {len(ctx.analysis_results)}
å¯è§†åŒ–å›¾è¡¨: {len(ctx.visualizations)}
å› å­å»ºè®®: {len(ctx.factor_proposals)}
æ‰§è¡Œæ­¥éª¤: {len(ctx.execution_log)}
"""
    path = ctx.save_session_log()
    report += f"ğŸ“„ æ—¥å¿—ä¿å­˜: {path}"
    ctx.log_function_call("generate_iteration_report", {"iteration_number": iteration_number}, report, 0)
    return report


# ============================================================================
# Alpha Research Agent ä¸»ç±»
# ============================================================================

class AlphaResearchAgent:
    """Alphaç ”ç©¶æ™ºèƒ½ä½“ - æ‰§è¡Œå®Œæ•´Alphaåˆ†ææµç¨‹"""

    def __init__(self):
        self.context = AlphaResearchContext()
        self.agent = Agent(
            name="AlphaResearchAgent",
            model="gpt-4o-mini",
            instructions="""
ä½ æ˜¯ä¸€åä¸“ä¸šçš„é‡åŒ–ç ”ç©¶åŠ©ç†ã€‚ä½ å¯ä»¥è°ƒç”¨å·¥å…·:
- load_and_analyze_data
- load_qlib_factors
- propose_alpha_factors
- generate_iteration_report
è¯·åŸºäºæ•°æ®è¿›è¡Œå®Œæ•´åˆ†æã€‚
""",
            tools=[
                load_and_analyze_data,
                load_qlib_factors,
                propose_alpha_factors,
                generate_iteration_report,
            ],
        )

    async def run_analysis(self, user_request: str):
        """å¼‚æ­¥è¿è¡Œåˆ†æä»»åŠ¡"""
        print(f"\nğŸš€ å¼€å§‹Alphaç ”ç©¶åˆ†æ | Session {self.context.session_id}\n")
        result_text = self.agent.run(user_request, context=self.context)
        log_path = self.context.save_session_log()
        print(f"\nâœ… åˆ†æå®Œæˆ | æ—¥å¿—: {log_path}\n")
        return result_text

    def run_analysis_sync(self, user_request: str):
        """åŒæ­¥æ‰§è¡Œç‰ˆæœ¬"""
        return asyncio.run(self.run_analysis(user_request))

    def summarize_with_llm(self):
        """
        ä½¿ç”¨ LLM å¯¹å½“å‰ context çš„åˆ†æç»“æœè¿›è¡Œæ€»ç»“ã€‚
        """
        # ä» context é‡Œæå–å…³é”®ä¿¡æ¯
        risk = self.context.analysis_results.get("risk_metrics", {})
        factor_df = self.context.analysis_results.get("qlib_factors")

        # ç”Ÿæˆæç¤ºè¯ï¼ˆpromptï¼‰
        text_prompt = f"""
ä½ æ˜¯ä¸€åé‡åŒ–ç ”ç©¶åŠ©ç†ã€‚ä»¥ä¸‹æ˜¯ä¸€æ¬¡Alphaç ”ç©¶ä»»åŠ¡çš„ç»“æœã€‚

ã€1ï¸âƒ£ æ•°æ®åˆ†æç»“æœã€‘
å¹´åŒ–æ³¢åŠ¨ç‡: {risk.get('volatility', 'æœªçŸ¥')}
å¤æ™®æ¯”ç‡: {risk.get('sharpe_ratio', 'æœªçŸ¥')}
å…¶å®ƒæŒ‡æ ‡: {risk}

ã€2ï¸âƒ£ Alphaå› å­è¡¨ç°ï¼ˆå‰10ä¸ªï¼‰ã€‘
{factor_df.head(10).to_string(index=False) if factor_df is not None else 'æš‚æ— å› å­æ•°æ®'}

è¯·ä½ å¸®æˆ‘å®Œæˆä»¥ä¸‹åˆ†æï¼š
- ç®€è¦æ€»ç»“å¸‚åœºç‰¹å¾ï¼›
- åˆ†æè¿™äº›å› å­çš„ä»£è¡¨æ„ä¹‰ï¼›
- æŒ‡å‡ºå“ªäº›å› å­ç»„åˆå¯èƒ½æœ‰ç”¨ï¼›
- ç»™å‡ºæ”¹è¿›æ–¹å‘æˆ–è¿›ä¸€æ­¥ç ”ç©¶å»ºè®®ã€‚
"""

        print("\nğŸ§  è°ƒç”¨ LLM è¿›è¡Œåˆ†ææ€»ç»“ ...")

        # ä½¿ç”¨ OpenAI å®¢æˆ·ç«¯è°ƒç”¨æ¨¡å‹
        try:
            response = self.agent.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é‡åŒ–ç ”ç©¶åˆ†æå¸ˆï¼Œæ“…é•¿Alphaå› å­åˆ†æã€‚"},
                    {"role": "user", "content": text_prompt},
                ],
            )
            summary_text = response.choices[0].message.content
            print("\nğŸ“Š LLM åˆ†æç»“æœ:\n", summary_text[:1500])
            return summary_text
        except Exception as e:
            print(f"âŒ LLM è°ƒç”¨å¤±è´¥: {e}")
            return f"âŒ LLM è°ƒç”¨å¤±è´¥: {e}"

    def run_complete_workflow(self, csv_path: str, user_input: str = ""):
        """
        å®Œæ•´æ‰§è¡ŒAlphaç ”ç©¶æµç¨‹ï¼š
        1. è°ƒç”¨å·¥å…·æ‰§è¡Œåˆ†æï¼›
        2. è°ƒç”¨LLMè¿›è¡Œæ€»ç»“åˆ†æï¼›
        3. è¾“å‡ºå®Œæ•´æŠ¥å‘Šã€‚
        """
        prompt = f"""
è¯·å¯¹ {csv_path} è¿›è¡Œå®Œæ•´alphaç ”ç©¶ï¼ŒåŒ…æ‹¬:
1. è°ƒç”¨ load_and_analyze_data
2. è°ƒç”¨ load_qlib_factors
3. è°ƒç”¨ propose_alpha_factors
4. ç”Ÿæˆç¬¬1æ¬¡è¿­ä»£æŠ¥å‘Š
{user_input}
"""

        # === (1) æ‰§è¡Œåˆ†æ ===
        base_result = self.run_analysis_sync(prompt)

        # === (2) è®© LLM æ€»ç»“ ===
        llm_summary = self.summarize_with_llm()

        # === (3) åˆå¹¶è¾“å‡º ===
        final_report = base_result + "\n\n======\nğŸ“ˆ LLMæ€»ç»“åˆ†æï¼š\n" + llm_summary
        print("\nâœ… å®Œæ•´æŠ¥å‘Šç”Ÿæˆå®Œæˆã€‚")
        return final_report


# ============================================================================
# ä¸»å…¥å£
# ============================================================================

def main():
    """æµ‹è¯•å…¥å£"""
    import qlib
    qlib.init(provider_uri="/content/AgenticTradng/qlib_data/stock_custom_day", region="us")
    agent = AlphaResearchAgent()
    report = agent.run_complete_workflow(
        "/content/AgenticTradng/qlib_data/stock_backup/XOM_daily.csv",
        user_input="è¿›è¡ŒæŠ€æœ¯åˆ†æä¸å› å­å»ºè®®"
    )

    # æ‰“å°æœ€ç»ˆæ€»ç»“
    print("\n\n============================")
    print("ğŸ“„ æœ€ç»ˆç»¼åˆæŠ¥å‘Šï¼ˆå«LLMåˆ†æï¼‰")
    print("============================\n")
    print(report)



if __name__ == "__main__":
    main()
