"""
Alpha Research Agent - åŸºäºOpenAI Agents SDKæ„å»º
é›†æˆAlphaAnalysisToolkitå’ŒAlphaVisualizationToolkit
"""
import os
import asyncio
import json
from typing import Dict, List, Any, Optional
from datetime import datetime
import pandas as pd
from pydantic import BaseModel

# OpenAI Agents SDK imports (æœ¬åœ°agents.pyå·²é‡å‘½åä¸ºlocal_agents.py)
from agents import Agent, Runner, function_tool, RunContextWrapper

# å¯¼å…¥å·¥å…·åŒ…
from alpha_analysis_toolkit import AlphaAnalysisToolkit
from alpha_visualization_toolkit import AlphaVisualizationToolkit


# ============================================================================
# Pydanticæ¨¡å‹å®šä¹‰ - ç”¨äºç»“æ„åŒ–è¾“å‡º
# ============================================================================

class FactorPerformance(BaseModel):
    """å› å­é¢„æœŸè¡¨ç°"""
    market_regime: str
    confidence_level: float
    expected_sharpe: float
    
    class Config:
        extra = "forbid"  # ç¦æ­¢é¢å¤–å­—æ®µ


class FactorProposal(BaseModel):
    """å•ä¸ªå› å­å»ºè®®"""
    factor_name: str
    description: str
    formula: str
    justification: str
    expected_performance: FactorPerformance
    
    class Config:
        extra = "forbid"  # ç¦æ­¢é¢å¤–å­—æ®µ


class AlphaFactorResponse(BaseModel):
    """LLMç»“æ„åŒ–å› å­å»ºè®®å“åº”"""
    factor_proposals: List[FactorProposal]
    market_summary: str
    risk_assessment: str
    
    class Config:
        extra = "forbid"  # ç¦æ­¢é¢å¤–å­—æ®µ


# ============================================================================
# Contextç±» - ç”¨äºåœ¨å·¥å…·é—´ä¼ é€’çŠ¶æ€
# ============================================================================

class AlphaResearchContext:
    """Alphaç ”ç©¶ä¸Šä¸‹æ–‡ - å­˜å‚¨åˆ†æçŠ¶æ€å’Œæ•°æ®"""
    def __init__(self):
        self.current_asset: Optional[str] = None
        self.market_data: Optional[pd.DataFrame] = None
        self.analysis_results: Dict[str, Any] = {}
        self.visualizations: Dict[str, Any] = {}
        self.factor_proposals: List[Dict] = []
        self.iteration_count: int = 0
        
        # æ‰§è¡Œæ—¥å¿—
        self.execution_log: List[Dict[str, Any]] = []
        self.session_id: str = datetime.now().strftime("%Y%m%d_%H%M%S")
        
    def log_function_call(self, function_name: str, args: Dict, result: str, 
                         execution_time: float = 0):
        """è®°å½•å‡½æ•°è°ƒç”¨"""
        log_entry = {
            'timestamp': datetime.now().isoformat(),
            'function_name': function_name,
            'arguments': args,
            'result_preview': result[:500] if len(result) > 500 else result,
            'result_length': len(result),
            'execution_time': execution_time,
            'step_number': len(self.execution_log) + 1
        }
        self.execution_log.append(log_entry)
        
    def save_session_log(self, output_dir: str = "agent_logs"):
        """ä¿å­˜sessionæ—¥å¿—åˆ°æ–‡ä»¶"""
        os.makedirs(output_dir, exist_ok=True)
        log_file = os.path.join(output_dir, f"session_{self.session_id}.json")
        
        log_data = {
            'session_id': self.session_id,
            'execution_log': self.execution_log,
            'summary': {
                'total_steps': len(self.execution_log),
                'current_asset': self.current_asset,
                'iteration_count': self.iteration_count,
                'has_data': self.market_data is not None,
                'analysis_results_count': len(self.analysis_results),
                'visualizations_count': len(self.visualizations),
                'factor_proposals_count': len(self.factor_proposals)
            }
        }
        
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(log_data, f, ensure_ascii=False, indent=2)
        
        return log_file


# ============================================================================
# è¾…åŠ©å‡½æ•°
# ============================================================================

def _calculate_factor_performance(factor_name: str, 
                                  data: pd.DataFrame,
                                  signals: Dict[str, pd.Series],
                                  risk_metrics: Dict[str, float]) -> Dict[str, Any]:
    """
    è®¡ç®—å› å­çš„é¢„æœŸè¡¨ç°æŒ‡æ ‡
    
    Args:
        factor_name: å› å­åç§°
        data: å¸‚åœºæ•°æ®
        signals: äº¤æ˜“ä¿¡å·å­—å…¸
        risk_metrics: é£é™©æŒ‡æ ‡
        
    Returns:
        åŒ…å«é¢„æœŸè¡¨ç°çš„å­—å…¸
    """
    import numpy as np
    
    result = {
        'justification': '',
        'market_regime': 'æœªçŸ¥',
        'confidence_level': 0.5,
        'expected_sharpe': 0.0,
        'ic_mean': 0.0,
        'ic_std': 0.0,
        'calculation_method': 'åŸºäºå†å²æ¨¡æ‹Ÿ'
    }
    
    try:
        # 1. ç¡®å®šå¸‚åœºç¯å¢ƒ
        volatility = risk_metrics.get('volatility', 0)
        mean_return = risk_metrics.get('mean_return', 0)
        
        if volatility > 0.25:
            market_regime = "é«˜æ³¢åŠ¨"
        elif volatility > 0.15:
            market_regime = "ä¸­æ³¢åŠ¨"
        else:
            market_regime = "ä½æ³¢åŠ¨"
            
        if mean_return > 0.1:
            market_regime += "ä¸Šæ¶¨"
        elif mean_return < -0.05:
            market_regime += "ä¸‹è·Œ"
        else:
            market_regime += "éœ‡è¡"
        
        result['market_regime'] = market_regime
        
        # 2. å°è¯•ä»signalsä¸­è·å–å¯¹åº”çš„å› å­ä¿¡å·
        factor_signal = None
        
        # åŒ¹é…å› å­åç§°
        for sig_name, sig_data in signals.items():
            if factor_name.lower() in sig_name.lower() or sig_name.lower() in factor_name.lower():
                factor_signal = sig_data
                break
        
        # 3. å¦‚æœæ‰¾åˆ°ä¿¡å·ï¼Œè®¡ç®—ICï¼ˆä¿¡æ¯ç³»æ•°ï¼‰å’Œé¢„æœŸSharpe
        if factor_signal is not None and len(factor_signal) > 20:
            # è®¡ç®—æ”¶ç›Šç‡
            if 'Returns' in data.columns:
                returns = data['Returns']
            elif 'Close' in data.columns:
                returns = data['Close'].pct_change()
            else:
                returns = None
            
            if returns is not None and len(returns) > 20:
                # å»é™¤NaN
                valid_idx = ~(factor_signal.isna() | returns.isna())
                clean_signal = factor_signal[valid_idx]
                clean_returns = returns[valid_idx]
                
                if len(clean_signal) > 20:
                    # è®¡ç®—æ»šåŠ¨ICï¼ˆä¿¡æ¯ç³»æ•° = å› å­å€¼ä¸æœªæ¥æ”¶ç›Šçš„ç›¸å…³æ€§ï¼‰
                    # ç®€åŒ–ç‰ˆæœ¬ï¼šè®¡ç®—å› å­ä¿¡å·ä¸ä¸‹ä¸€æœŸæ”¶ç›Šçš„ç›¸å…³æ€§
                    future_returns = clean_returns.shift(-1)
                    
                    # è®¡ç®—åˆ†æ®µIC
                    window_size = min(20, len(clean_signal) // 5)
                    ic_values = []
                    
                    for i in range(0, len(clean_signal) - window_size, window_size):
                        window_signal = clean_signal.iloc[i:i+window_size]
                        window_future_ret = future_returns.iloc[i:i+window_size]
                        
                        if len(window_signal) > 5 and not window_future_ret.isna().all():
                            ic = window_signal.corr(window_future_ret)
                            if not np.isnan(ic):
                                ic_values.append(ic)
                    
                    if ic_values:
                        ic_mean = np.mean(ic_values)
                        ic_std = np.std(ic_values)
                        
                        # è®¡ç®—é¢„æœŸSharpeï¼šç®€åŒ–å…¬å¼ Sharpe â‰ˆ IC_mean * sqrt(N) / IC_std
                        # å…¶ä¸­Næ˜¯å¹´åŒ–äº¤æ˜“æ¬¡æ•°çš„å¹³æ–¹æ ¹
                        trading_days_per_year = 252
                        # åˆ¤æ–­æ•°æ®é¢‘ç‡ï¼ˆé¿å…ä½¿ç”¨.freqå±æ€§ï¼Œå®ƒå¯èƒ½ä¸å­˜åœ¨ï¼‰
                        try:
                            freq_str = str(data.index.freq) if hasattr(data.index, 'freq') else ''
                        except:
                            freq_str = ''
                        
                        if 'hourly' in freq_str.lower() or 'h' in freq_str.lower() or len(data) > 1000:
                            # é«˜é¢‘æ•°æ®
                            sqrt_n = np.sqrt(trading_days_per_year * 6)  # å‡è®¾æ¯å¤©6å°æ—¶äº¤æ˜“
                        else:
                            sqrt_n = np.sqrt(trading_days_per_year)
                        
                        if ic_std > 0:
                            expected_sharpe = abs(ic_mean) * sqrt_n / ic_std
                        else:
                            expected_sharpe = abs(ic_mean) * sqrt_n
                        
                        # é™åˆ¶åœ¨åˆç†èŒƒå›´
                        expected_sharpe = min(max(expected_sharpe, -2.0), 3.0)
                        
                        result['ic_mean'] = float(ic_mean)
                        result['ic_std'] = float(ic_std)
                        result['expected_sharpe'] = float(expected_sharpe)
                        
                        # è®¡ç®—ä¿¡å¿ƒæ°´å¹³ï¼ˆåŸºäºICçš„ç¨³å®šæ€§ï¼‰
                        if len(ic_values) >= 10:
                            # ICå€¼çš„ä¸€è‡´æ€§
                            positive_ic_ratio = sum(1 for ic in ic_values if ic > 0) / len(ic_values)
                            confidence = 0.5 + 0.3 * abs(ic_mean) + 0.2 * positive_ic_ratio
                            result['confidence_level'] = min(confidence, 0.95)
                        else:
                            result['confidence_level'] = 0.5
                        
                        # ç”Ÿæˆç†ç”±
                        result['justification'] = (
                            f"åŸºäºå†å²å›æµ‹ï¼Œè¯¥å› å­çš„å¹³å‡ICä¸º{ic_mean:.3f}ï¼ˆæ ‡å‡†å·®{ic_std:.3f}ï¼‰ï¼Œ"
                            f"æ˜¾ç¤º{'æ­£å‘' if ic_mean > 0 else 'è´Ÿå‘'}é¢„æµ‹èƒ½åŠ›ã€‚"
                            f"åœ¨{market_regime}ç¯å¢ƒä¸‹ï¼Œé¢„æœŸå¹´åŒ–Sharpeæ¯”ç‡çº¦ä¸º{expected_sharpe:.2f}ã€‚"
                        )
                        result['calculation_method'] = f"æ»šåŠ¨ICæ³•ï¼ˆçª—å£={window_size}ï¼Œæ ·æœ¬æ•°={len(ic_values)}ï¼‰"
                        
                        return result
        
        # 4. å¦‚æœæ— æ³•è®¡ç®—ï¼Œä½¿ç”¨åŸºäºå› å­ç±»å‹çš„ç»éªŒä¼°è®¡
        result['justification'] = f"åŸºäº{factor_name}å› å­ç±»å‹å’Œå½“å‰{market_regime}ç¯å¢ƒçš„ç»éªŒä¼°è®¡"
        result['calculation_method'] = "ç»éªŒä¼°è®¡ï¼ˆæ— å†å²ä¿¡å·æ•°æ®ï¼‰"
        
        # æ ¹æ®å› å­ç±»å‹ç»™å‡ºç»éªŒSharpe
        if 'momentum' in factor_name.lower():
            if 'trend' in market_regime or 'ä¸Šæ¶¨' in market_regime:
                result['expected_sharpe'] = 1.2
                result['confidence_level'] = 0.7
            else:
                result['expected_sharpe'] = 0.6
                result['confidence_level'] = 0.5
        elif 'revers' in factor_name.lower() or 'mean' in factor_name.lower():
            if 'éœ‡è¡' in market_regime:
                result['expected_sharpe'] = 1.0
                result['confidence_level'] = 0.65
            else:
                result['expected_sharpe'] = 0.5
                result['confidence_level'] = 0.45
        elif 'volatility' in factor_name.lower() or 'vol' in factor_name.lower():
            if 'é«˜æ³¢åŠ¨' in market_regime:
                result['expected_sharpe'] = 0.9
                result['confidence_level'] = 0.6
            else:
                result['expected_sharpe'] = 0.7
                result['confidence_level'] = 0.55
        elif 'trend' in factor_name.lower():
            if 'trend' in market_regime or 'ä¸Šæ¶¨' in market_regime or 'ä¸‹è·Œ' in market_regime:
                result['expected_sharpe'] = 1.3
                result['confidence_level'] = 0.75
            else:
                result['expected_sharpe'] = 0.4
                result['confidence_level'] = 0.4
        else:
            # é»˜è®¤ä¼°è®¡
            result['expected_sharpe'] = 0.8
            result['confidence_level'] = 0.5
        
        result['justification'] += f"ã€‚å½“å‰å¸‚åœºç¯å¢ƒä¸º{market_regime}ï¼Œè¯¥ç±»å‹å› å­é€šå¸¸è¡¨ç°{'è¾ƒå¥½' if result['expected_sharpe'] > 0.8 else 'ä¸€èˆ¬'}ã€‚"
        
    except Exception as e:
        result['justification'] = f"å› å­è¯„ä¼°é‡åˆ°é”™è¯¯: {str(e)}ï¼Œä½¿ç”¨é»˜è®¤ä¼°è®¡"
        result['expected_sharpe'] = 0.5
        result['confidence_level'] = 0.3
        result['calculation_method'] = "é»˜è®¤å€¼ï¼ˆè®¡ç®—å¤±è´¥ï¼‰"
    
    return result


# ============================================================================
# å·¥å…·å‡½æ•° - ä½¿ç”¨@function_toolè£…é¥°å™¨
# ============================================================================

@function_tool
def load_and_analyze_data(ctx: RunContextWrapper[AlphaResearchContext], 
                          csv_path: str, 
                          qlib_format: bool = False) -> str:
    """
    åŠ è½½å¹¶åˆ†æèµ„äº§æ•°æ®ï¼Œè®¡ç®—æŠ€æœ¯æŒ‡æ ‡å’Œä¿¡å·
    
    Args:
        csv_path: CSVæ–‡ä»¶è·¯å¾„æˆ–qlibæ•°æ®ç›®å½•
        qlib_format: æ˜¯å¦ä½¿ç”¨qlibæ ¼å¼
        
    Returns:
        åˆ†ææ‘˜è¦å­—ç¬¦ä¸²
    """
    start_time = datetime.now()
    try:
        print(f"\nğŸ”§ [STEP {len(ctx.context.execution_log)+1}] è°ƒç”¨: load_and_analyze_data")
        print(f"   å‚æ•°: csv_path={csv_path}, qlib_format={qlib_format}")
        
        # 1. åŠ è½½æ•°æ®
        data = AlphaAnalysisToolkit.load_asset_data(csv_path, data_format="csv")
        data = AlphaAnalysisToolkit.preprocess_data(data, qlib_format=qlib_format)
        
        # 2. è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
        indicators = AlphaAnalysisToolkit.calculate_technical_indicators(data, qlib_format=qlib_format)
        
        # 3. ç”Ÿæˆäº¤æ˜“ä¿¡å·
        signals = AlphaAnalysisToolkit.generate_alpha_signals(data, qlib_format=qlib_format)
        
        # 4. è®¡ç®—é£é™©æŒ‡æ ‡
        risk_metrics = AlphaAnalysisToolkit.calculate_risk_metrics(data, qlib_format=qlib_format)
        
        # 5. å­˜å‚¨åˆ°context
        ctx.context.current_asset = csv_path
        ctx.context.market_data = data
        ctx.context.analysis_results = {
            'technical_indicators': indicators,
            'signals': signals,
            'risk_metrics': risk_metrics,
            'data_summary': {
                'rows': len(data),
                'columns': list(data.columns),
                'date_range': f"{data.index.min()} to {data.index.max()}"
            }
        }
        
        # 6. ç”Ÿæˆæ‘˜è¦
        summary = f"""
âœ… æ•°æ®åŠ è½½æˆåŠŸ: {csv_path}
ğŸ“Š æ•°æ®è¡Œæ•°: {len(data)}
ğŸ“… æ—¶é—´èŒƒå›´: {data.index.min()} è‡³ {data.index.max()}
ğŸ“ˆ æŠ€æœ¯æŒ‡æ ‡: {len(indicators)} ä¸ª
ğŸ¯ äº¤æ˜“ä¿¡å·: {len(signals)} ä¸ª
âš ï¸ é£é™©æŒ‡æ ‡: å¹´åŒ–æ³¢åŠ¨ç‡={risk_metrics.get('volatility', 0):.2%}, Sharpeæ¯”ç‡={risk_metrics.get('sharpe_ratio', 0):.2f}
"""
        
        # è®°å½•æ‰§è¡Œ
        execution_time = (datetime.now() - start_time).total_seconds()
        ctx.context.log_function_call(
            'load_and_analyze_data',
            {'csv_path': csv_path, 'qlib_format': qlib_format},
            summary,
            execution_time
        )
        print(f"   âœ… æ‰§è¡Œå®Œæˆ (è€—æ—¶: {execution_time:.2f}ç§’)")
        
        return summary
        
    except Exception as e:
        error_msg = f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {str(e)}"
        execution_time = (datetime.now() - start_time).total_seconds()
        ctx.context.log_function_call(
            'load_and_analyze_data',
            {'csv_path': csv_path, 'qlib_format': qlib_format},
            error_msg,
            execution_time
        )
        print(f"   âŒ æ‰§è¡Œå¤±è´¥ (è€—æ—¶: {execution_time:.2f}ç§’)")
        return error_msg


@function_tool
def create_visualizations(ctx: RunContextWrapper[AlphaResearchContext],
                         chart_types: str = "performance,factor_analysis") -> str:
    """
    åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
    
    Args:
        chart_types: å›¾è¡¨ç±»å‹å­—ç¬¦ä¸²ï¼Œç”¨é€—å·åˆ†éš”ï¼Œå¦‚ "performance,heatmap,regime"
        
    Returns:
        å¯è§†åŒ–åˆ›å»ºç»“æœ
    """
    start_time = datetime.now()
    print(f"\nğŸ”§ [STEP {len(ctx.context.execution_log)+1}] è°ƒç”¨: create_visualizations")
    print(f"   å‚æ•°: chart_types={chart_types}")
    
    if ctx.context.market_data is None:
        error_msg = "âŒ è¯·å…ˆåŠ è½½æ•°æ® (ä½¿ç”¨load_and_analyze_dataå·¥å…·)"
        ctx.context.log_function_call('create_visualizations', {'chart_types': chart_types}, error_msg, 0)
        return error_msg
    
    # è§£æå›¾è¡¨ç±»å‹
    if isinstance(chart_types, str):
        chart_types = [t.strip() for t in chart_types.split(',')]
    elif chart_types is None:
        chart_types = ["performance", "factor_analysis"]
    
    try:
        viz_toolkit = AlphaVisualizationToolkit(theme="alpha_dark")
        data = ctx.context.market_data
        visualizations = {}
        
        # æ ¹æ®è¯·æ±‚åˆ›å»ºå›¾è¡¨
        if "performance" in chart_types and 'Returns' in data.columns:
            fig = viz_toolkit.create_performance_attribution(data[['Returns']])
            visualizations['performance'] = fig
            
        if "heatmap" in chart_types and data.shape[1] > 2:
            fig = viz_toolkit.create_factor_heatmap(data)
            visualizations['heatmap'] = fig
            
        if "regime" in chart_types and 'Close' in data.columns:
            fig = viz_toolkit.create_regime_detection_chart(data['Close'])
            visualizations['regime'] = fig
            
        if "factor_analysis" in chart_types:
            signals = ctx.context.analysis_results.get('signals', {})
            if signals and len(signals) > 0:
                # å°†signalsæ·»åŠ åˆ°dataä¸­ä»¥ä¾¿å¯è§†åŒ–
                for name, signal in signals.items():
                    if isinstance(signal, pd.Series) and len(signal) == len(data):
                        data[name] = signal
                
                factor_names = list(signals.keys())[:5]  # æœ€å¤š5ä¸ªå› å­
                fig = viz_toolkit.create_alpha_factor_analysis(data, factor_names)
                visualizations['factor_analysis'] = fig
        
        # å­˜å‚¨åˆ°context
        ctx.context.visualizations = visualizations
        
        # å¯¼å‡ºå›¾è¡¨
        if visualizations:
            charts_list = list(visualizations.values())
            export_summary = viz_toolkit.export_charts_for_multimodal_input(charts_list)
            
            result = f"""
âœ… åˆ›å»ºäº† {len(visualizations)} ä¸ªå¯è§†åŒ–å›¾è¡¨
ğŸ“ å¯¼å‡ºæ–‡ä»¶:
  - PNG: {len(export_summary.get('png_files', []))} ä¸ª
  - HTML: {len(export_summary.get('html_files', []))} ä¸ª
  - JSON: {len(export_summary.get('json_files', []))} ä¸ª
ğŸ“Š å›¾è¡¨ç±»å‹: {', '.join(visualizations.keys())}
"""
            execution_time = (datetime.now() - start_time).total_seconds()
            ctx.context.log_function_call(
                'create_visualizations',
                {'chart_types': chart_types},
                result,
                execution_time
            )
            print(f"   âœ… æ‰§è¡Œå®Œæˆ (è€—æ—¶: {execution_time:.2f}ç§’)")
            return result
        else:
            result = "âš ï¸ æœªåˆ›å»ºä»»ä½•å›¾è¡¨"
            execution_time = (datetime.now() - start_time).total_seconds()
            ctx.context.log_function_call('create_visualizations', {'chart_types': chart_types}, result, execution_time)
            print(f"   âš ï¸ æ‰§è¡Œå®Œæˆ (è€—æ—¶: {execution_time:.2f}ç§’)")
            return result
            
    except Exception as e:
        error_msg = f"âŒ å¯è§†åŒ–åˆ›å»ºå¤±è´¥: {str(e)}"
        execution_time = (datetime.now() - start_time).total_seconds()
        ctx.context.log_function_call('create_visualizations', {'chart_types': chart_types}, error_msg, execution_time)
        print(f"   âŒ æ‰§è¡Œå¤±è´¥ (è€—æ—¶: {execution_time:.2f}ç§’)")
        return error_msg


@function_tool
def propose_alpha_factors(ctx: RunContextWrapper[AlphaResearchContext],
                         user_input: str = "") -> str:
    """
    åŸºäºå¸‚åœºåˆ†ææå‡ºalphaå› å­å»ºè®®
    
    Args:
        user_input: ç”¨æˆ·çš„è¡¥å……è¾“å…¥æˆ–ç‰¹æ®Šè¦æ±‚
        
    Returns:
        å› å­å»ºè®®åˆ—è¡¨
    """
    start_time = datetime.now()
    print(f"\nğŸ”§ [STEP {len(ctx.context.execution_log)+1}] è°ƒç”¨: propose_alpha_factors")
    print(f"   å‚æ•°: user_input={user_input if user_input else '(empty)'}")
    
    if ctx.context.market_data is None:
        error_msg = "âŒ è¯·å…ˆåŠ è½½æ•°æ® (ä½¿ç”¨load_and_analyze_dataå·¥å…·)"
        ctx.context.log_function_call('propose_alpha_factors', {'user_input': user_input}, error_msg, 0)
        return error_msg
    
    try:
        # ä½¿ç”¨å·¥å…·åŒ…çš„å› å­å»ºè®®åŠŸèƒ½
        data = ctx.context.market_data
        signals = ctx.context.analysis_results.get('signals', {})
        risk_metrics = ctx.context.analysis_results.get('risk_metrics', {})
        factors = AlphaAnalysisToolkit.propose_alpha_factors(data, qlib_format=False)
        
        # æ„å»ºç»“æ„åŒ–å»ºè®®ï¼ˆè®¡ç®—çœŸå®çš„é¢„æœŸè¡¨ç°ï¼‰
        proposals = []
        for i, factor in enumerate(factors, 1):
            # è®¡ç®—å› å­çš„é¢„æœŸè¡¨ç°
            factor_performance = _calculate_factor_performance(
                factor, data, signals, risk_metrics
            )
            
            proposal = {
                'factor_name': factor,
                'description': f"åŸºäºå¸‚åœºæ•°æ®åˆ†æå»ºè®®çš„{factor}å› å­",
                'formula': f"Ref({factor}, -1) / Ref({factor}, -5) - 1",  # ç¤ºä¾‹å…¬å¼
                'justification': factor_performance['justification'],
                'expected_performance': {
                    'market_regime': factor_performance['market_regime'],
                    'confidence_level': factor_performance['confidence_level'],
                    'expected_sharpe': factor_performance['expected_sharpe'],
                    'ic_mean': factor_performance.get('ic_mean', 0),
                    'ic_std': factor_performance.get('ic_std', 0),
                    'calculation_method': factor_performance['calculation_method']
                }
            }
            proposals.append(proposal)
        
        # å­˜å‚¨åˆ°context
        ctx.context.factor_proposals = proposals
        
        # æ ¼å¼åŒ–è¾“å‡º
        result = f"\n{'='*60}\n"
        result += f"ğŸ¯ Alphaå› å­å»ºè®® (åŸºäºæ•°æ®åˆ†æ)\n"
        result += f"{'='*60}\n\n"
        
        if user_input:
            result += f"ğŸ’¡ ç”¨æˆ·éœ€æ±‚: {user_input}\n\n"
        
        for i, proposal in enumerate(proposals, 1):
            result += f"{i}. **{proposal['factor_name']}**\n"
            result += f"   æè¿°: {proposal['description']}\n"
            result += f"   å…¬å¼: {proposal['formula']}\n"
            result += f"   ç†ç”±: {proposal['justification']}\n"
            result += f"   é¢„æœŸè¡¨ç°: Sharpe={proposal['expected_performance']['expected_sharpe']:.2f}\n\n"
        
        execution_time = (datetime.now() - start_time).total_seconds()
        ctx.context.log_function_call(
            'propose_alpha_factors',
            {'user_input': user_input},
            result,
            execution_time
        )
        print(f"   âœ… æ‰§è¡Œå®Œæˆ (è€—æ—¶: {execution_time:.2f}ç§’, ç”Ÿæˆ{len(proposals)}ä¸ªå› å­)")
        
        return result
        
    except Exception as e:
        error_msg = f"âŒ å› å­å»ºè®®ç”Ÿæˆå¤±è´¥: {str(e)}"
        execution_time = (datetime.now() - start_time).total_seconds()
        ctx.context.log_function_call('propose_alpha_factors', {'user_input': user_input}, error_msg, execution_time)
        print(f"   âŒ æ‰§è¡Œå¤±è´¥ (è€—æ—¶: {execution_time:.2f}ç§’)")
        return error_msg


@function_tool
def analyze_backtest_results(ctx: RunContextWrapper[AlphaResearchContext],
                             total_return: float = 0.0,
                             sharpe_ratio: float = 0.0,
                             max_drawdown: float = 0.0,
                             win_rate: float = 0.0) -> str:
    """
    åˆ†æå›æµ‹ç»“æœå¹¶æä¾›æ”¹è¿›å»ºè®®
    
    Args:
        total_return: æ€»æ”¶ç›Šç‡
        sharpe_ratio: Sharpeæ¯”ç‡
        max_drawdown: æœ€å¤§å›æ’¤
        win_rate: èƒœç‡
        
    Returns:
        åˆ†æç»“æœå’Œæ”¹è¿›å»ºè®®
    """
    start_time = datetime.now()
    print(f"\nğŸ”§ [STEP {len(ctx.context.execution_log)+1}] è°ƒç”¨: analyze_backtest_results")
    print(f"   å‚æ•°: return={total_return}, sharpe={sharpe_ratio}, dd={max_drawdown}, wr={win_rate}")
    
    try:
        analysis = "\n" + "="*60 + "\n"
        analysis += "ğŸ“Š å›æµ‹ç»“æœåˆ†æ\n"
        analysis += "="*60 + "\n\n"
        
        analysis += f"ğŸ“ˆ æ€»æ”¶ç›Šç‡: {total_return:.2%}\n"
        analysis += f"ğŸ“Š Sharpeæ¯”ç‡: {sharpe_ratio:.2f}\n"
        analysis += f"ğŸ“‰ æœ€å¤§å›æ’¤: {max_drawdown:.2%}\n"
        analysis += f"ğŸ¯ èƒœç‡: {win_rate:.2%}\n\n"
        
        # æä¾›æ”¹è¿›å»ºè®®
        analysis += "ğŸ’¡ æ”¹è¿›å»ºè®®:\n"
        
        if sharpe_ratio < 1.0:
            analysis += "  - Sharpeæ¯”ç‡åä½ï¼Œè€ƒè™‘ä¼˜åŒ–å› å­æƒé‡æˆ–é£é™©æ§åˆ¶\n"
        
        if abs(max_drawdown) > 0.2:
            analysis += "  - æœ€å¤§å›æ’¤è¾ƒå¤§ï¼Œå»ºè®®åŠ å¼ºæ­¢æŸæœºåˆ¶\n"
        
        if win_rate < 0.5:
            analysis += "  - èƒœç‡åä½ï¼Œè€ƒè™‘è°ƒæ•´å…¥åœºä¿¡å·é˜ˆå€¼\n"
        
        if total_return < 0:
            analysis += "  - ç­–ç•¥è¡¨ç°ä¸ºè´Ÿï¼Œéœ€è¦é‡æ–°è¯„ä¼°å› å­æœ‰æ•ˆæ€§\n"
        else:
            analysis += "  - ç­–ç•¥æ•´ä½“è¡¨ç°è‰¯å¥½ï¼Œç»§ç»­ç›‘æ§å®ç›˜è¡¨ç°\n"
        
        execution_time = (datetime.now() - start_time).total_seconds()
        ctx.context.log_function_call(
            'analyze_backtest_results',
            {
                'total_return': total_return,
                'sharpe_ratio': sharpe_ratio,
                'max_drawdown': max_drawdown,
                'win_rate': win_rate
            },
            analysis,
            execution_time
        )
        print(f"   âœ… æ‰§è¡Œå®Œæˆ (è€—æ—¶: {execution_time:.2f}ç§’)")
        
        return analysis
        
    except Exception as e:
        error_msg = f"âŒ å›æµ‹åˆ†æå¤±è´¥: {str(e)}"
        execution_time = (datetime.now() - start_time).total_seconds()
        ctx.context.log_function_call('analyze_backtest_results', {}, error_msg, execution_time)
        print(f"   âŒ æ‰§è¡Œå¤±è´¥ (è€—æ—¶: {execution_time:.2f}ç§’)")
        return error_msg


@function_tool
def generate_iteration_report(ctx: RunContextWrapper[AlphaResearchContext],
                              iteration_number: int) -> str:
    """
    ç”Ÿæˆç ”ç©¶è¿­ä»£æŠ¥å‘Š
    
    Args:
        iteration_number: è¿­ä»£æ¬¡æ•°
        
    Returns:
        è¿­ä»£æŠ¥å‘Š
    """
    start_time = datetime.now()
    print(f"\nğŸ”§ [STEP {len(ctx.context.execution_log)+1}] è°ƒç”¨: generate_iteration_report")
    print(f"   å‚æ•°: iteration_number={iteration_number}")
    
    ctx.context.iteration_count = iteration_number
    
    report = "\n" + "="*60 + "\n"
    report += f"ğŸ“‹ ç ”ç©¶è¿­ä»£æŠ¥å‘Š #{iteration_number}\n"
    report += "="*60 + "\n\n"
    
    report += f"ğŸ“ å½“å‰èµ„äº§: {ctx.context.current_asset or 'æœªåŠ è½½'}\n"
    report += f"ğŸ“Š æ•°æ®çŠ¶æ€: {'å·²åŠ è½½' if ctx.context.market_data is not None else 'æœªåŠ è½½'}\n"
    report += f"ğŸ”¬ åˆ†æç»“æœ: {len(ctx.context.analysis_results)} é¡¹\n"
    report += f"ğŸ“ˆ å¯è§†åŒ–å›¾è¡¨: {len(ctx.context.visualizations)} ä¸ª\n"
    report += f"ğŸ¯ å› å­å»ºè®®: {len(ctx.context.factor_proposals)} ä¸ª\n\n"
    
    report += "âœ¨ ç ”ç©¶è´¨é‡è¯„åˆ†: "
    quality_score = 0
    if ctx.context.market_data is not None:
        quality_score += 25
    if ctx.context.analysis_results:
        quality_score += 25
    if ctx.context.visualizations:
        quality_score += 25
    if ctx.context.factor_proposals:
        quality_score += 25
    
    report += f"{quality_score}/100\n\n"
    
    # æ·»åŠ æ‰§è¡Œæ—¥å¿—æ‘˜è¦
    report += f"ğŸ” æ‰§è¡Œæ­¥éª¤: {len(ctx.context.execution_log)} ä¸ªå‡½æ•°è°ƒç”¨\n"
    for log in ctx.context.execution_log:
        report += f"   [{log['step_number']}] {log['function_name']} (è€—æ—¶: {log['execution_time']:.2f}s)\n"
    
    execution_time = (datetime.now() - start_time).total_seconds()
    ctx.context.log_function_call(
        'generate_iteration_report',
        {'iteration_number': iteration_number},
        report,
        execution_time
    )
    print(f"   âœ… æ‰§è¡Œå®Œæˆ (è€—æ—¶: {execution_time:.2f}ç§’)")
    
    # ä¿å­˜æ—¥å¿—æ–‡ä»¶
    log_file = ctx.context.save_session_log()
    report += f"\nğŸ“„ å®Œæ•´æ‰§è¡Œæ—¥å¿—å·²ä¿å­˜è‡³: {log_file}\n"
    print(f"   ğŸ“„ æ—¥å¿—æ–‡ä»¶: {log_file}")
    
    return report


# ============================================================================
# Alpha Research Agentç±»
# ============================================================================

class AlphaResearchAgent:
    """
    Alphaç ”ç©¶æ™ºèƒ½ä½“ - åŸºäºOpenAI Agents SDK
    è´Ÿè´£é‡åŒ–äº¤æ˜“ç­–ç•¥å¼€å‘å’Œåˆ†æ
    """
    
    def __init__(self):
        """åˆå§‹åŒ–Agent"""
        # åˆ›å»ºcontextå®ä¾‹
        self.context = AlphaResearchContext()
        
        # åˆ›å»ºAgentå®ä¾‹
        self.agent = Agent(
            name="AlphaResearchAgent",
            instructions="""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡åŒ–äº¤æ˜“ç ”ç©¶å‘˜ï¼Œä¸“æ³¨äºalphaå› å­ç ”ç©¶å’Œç­–ç•¥å¼€å‘ã€‚

ä½ çš„ä¸»è¦èŒè´£:
1. åŠ è½½å’Œåˆ†æå¸‚åœºæ•°æ®
2. è®¡ç®—æŠ€æœ¯æŒ‡æ ‡å’Œé£é™©æŒ‡æ ‡
3. ç”Ÿæˆäº¤æ˜“ä¿¡å·
4. åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
5. æå‡ºalphaå› å­å»ºè®®
6. åˆ†æå›æµ‹ç»“æœ

å·¥ä½œæµç¨‹:
1. é¦–å…ˆä½¿ç”¨load_and_analyze_dataåŠ è½½æ•°æ®
2. ç„¶åä½¿ç”¨create_visualizationsåˆ›å»ºå›¾è¡¨
3. ä½¿ç”¨propose_alpha_factorsæå‡ºå› å­å»ºè®®
4. å¦‚æœæœ‰å›æµ‹ç»“æœï¼Œä½¿ç”¨analyze_backtest_resultsåˆ†æ
5. æœ€åä½¿ç”¨generate_iteration_reportç”ŸæˆæŠ¥å‘Š

å§‹ç»ˆä¿æŒä¸“ä¸šã€è¯¦ç»†å’Œæ•°æ®é©±åŠ¨çš„åˆ†ææ–¹å¼ã€‚
""",
            model="o4-mini",
            tools=[
                load_and_analyze_data,
                create_visualizations,
                propose_alpha_factors,
                analyze_backtest_results,
                generate_iteration_report
            ]
        )
    
    async def run_analysis(self, user_request: str) -> str:
        """
        è¿è¡Œåˆ†æä»»åŠ¡
        
        Args:
            user_request: ç”¨æˆ·è¯·æ±‚
            
        Returns:
            åˆ†æç»“æœ
        """
        try:
            print(f"\n{'='*70}")
            print(f"ğŸš€ å¼€å§‹Alphaç ”ç©¶åˆ†æ")
            print(f"   Session ID: {self.context.session_id}")
            print(f"{'='*70}\n")
            
            result = await Runner.run(
                self.agent,
                user_request,
                context=self.context,
                max_turns=10
            )
            
            # ä¿å­˜æœ€ç»ˆæ‰§è¡Œæ—¥å¿—
            log_file = self.context.save_session_log()
            
            print(f"\n{'='*70}")
            print(f"âœ… åˆ†æå®Œæˆ")
            print(f"   æ€»æ‰§è¡Œæ­¥éª¤: {len(self.context.execution_log)}")
            print(f"   æ—¥å¿—æ–‡ä»¶: {log_file}")
            print(f"{'='*70}\n")
            
            return result.final_output
        except Exception as e:
            error_msg = f"âŒ åˆ†æå¤±è´¥: {str(e)}"
            # å³ä½¿å¤±è´¥ä¹Ÿä¿å­˜æ—¥å¿—
            try:
                log_file = self.context.save_session_log()
                print(f"   é”™è¯¯æ—¥å¿—å·²ä¿å­˜: {log_file}")
            except:
                pass
            return error_msg
    
    def run_analysis_sync(self, user_request: str) -> str:
        """
        åŒæ­¥è¿è¡Œåˆ†æä»»åŠ¡
        
        Args:
            user_request: ç”¨æˆ·è¯·æ±‚
            
        Returns:
            åˆ†æç»“æœ
        """
        return asyncio.run(self.run_analysis(user_request))
    
    def run_complete_workflow(self, csv_path: str, user_input: str = "") -> str:
        """
        è¿è¡Œå®Œæ•´çš„åˆ†æå·¥ä½œæµ
        
        Args:
            csv_path: CSVæ–‡ä»¶è·¯å¾„
            user_input: ç”¨æˆ·è¡¥å……è¾“å…¥
            
        Returns:
            å®Œæ•´å·¥ä½œæµç»“æœ
        """
        request = f"""
è¯·å¯¹æ–‡ä»¶ {csv_path} è¿›è¡Œå®Œæ•´çš„alphaç ”ç©¶åˆ†æ:

1. åŠ è½½å¹¶åˆ†ææ•°æ®
2. åˆ›å»ºå¯è§†åŒ–å›¾è¡¨ï¼ˆåŒ…æ‹¬performanceã€factor_analysisã€regimeï¼‰
3. æå‡ºalphaå› å­å»ºè®®
4. ç”Ÿæˆç¬¬1æ¬¡è¿­ä»£æŠ¥å‘Š

ç”¨æˆ·éœ€æ±‚: {user_input if user_input else 'æ ‡å‡†åˆ†ææµç¨‹'}
"""
        return self.run_analysis_sync(request)
    
    def print_execution_log(self, detailed: bool = True):
        """
        æ‰“å°æ‰§è¡Œæ—¥å¿—
        
        Args:
            detailed: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        """
        print("\n" + "="*70)
        print(f"ğŸ“Š æ‰§è¡Œæ—¥å¿— - Session: {self.context.session_id}")
        print("="*70 + "\n")
        
        if not self.context.execution_log:
            print("âš ï¸ æš‚æ— æ‰§è¡Œè®°å½•\n")
            return
        
        for log in self.context.execution_log:
            print(f"[æ­¥éª¤ {log['step_number']}] {log['function_name']}")
            print(f"   æ—¶é—´: {log['timestamp']}")
            print(f"   è€—æ—¶: {log['execution_time']:.2f}ç§’")
            print(f"   å‚æ•°: {log['arguments']}")
            
            if detailed:
                print(f"   ç»“æœé¢„è§ˆ:")
                preview = log['result_preview']
                for line in preview.split('\n')[:10]:  # åªæ˜¾ç¤ºå‰10è¡Œ
                    print(f"      {line}")
                if log['result_length'] > 500:
                    print(f"      ... (å…±{log['result_length']}å­—ç¬¦)")
            
            print()
        
        print(f"æ€»è®¡: {len(self.context.execution_log)} ä¸ªå‡½æ•°è°ƒç”¨\n")
    
    def save_final_report(self, output_file: str = None):
        """
        ä¿å­˜æœ€ç»ˆæŠ¥å‘Šåˆ°Markdownæ–‡ä»¶
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        if output_file is None:
            output_dir = "agent_reports"
            os.makedirs(output_dir, exist_ok=True)
            output_file = os.path.join(output_dir, f"report_{self.context.session_id}.md")
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(f"# Alpha Research Report\n\n")
            f.write(f"**Session ID**: {self.context.session_id}\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write(f"## æ‰§è¡Œæ‘˜è¦\n\n")
            f.write(f"- å½“å‰èµ„äº§: {self.context.current_asset or 'N/A'}\n")
            f.write(f"- æ•°æ®çŠ¶æ€: {'âœ… å·²åŠ è½½' if self.context.market_data is not None else 'âŒ æœªåŠ è½½'}\n")
            f.write(f"- åˆ†æé¡¹ç›®: {len(self.context.analysis_results)}\n")
            f.write(f"- å¯è§†åŒ–å›¾è¡¨: {len(self.context.visualizations)}\n")
            f.write(f"- å› å­å»ºè®®: {len(self.context.factor_proposals)}\n")
            f.write(f"- æ‰§è¡Œæ­¥éª¤: {len(self.context.execution_log)}\n\n")
            
            f.write(f"## æ‰§è¡Œæ—¥å¿—\n\n")
            for i, log in enumerate(self.context.execution_log, 1):
                f.write(f"### æ­¥éª¤ {i}: {log['function_name']}\n\n")
                f.write(f"- **æ—¶é—´**: {log['timestamp']}\n")
                f.write(f"- **è€—æ—¶**: {log['execution_time']:.2f}ç§’\n")
                f.write(f"- **å‚æ•°**: `{log['arguments']}`\n")
                f.write(f"- **ç»“æœé•¿åº¦**: {log['result_length']} å­—ç¬¦\n\n")
                f.write(f"**ç»“æœé¢„è§ˆ**:\n```\n{log['result_preview']}\n```\n\n")
            
            if self.context.factor_proposals:
                f.write(f"## Alphaå› å­å»ºè®®\n\n")
                for i, factor in enumerate(self.context.factor_proposals, 1):
                    f.write(f"### {i}. {factor['factor_name']}\n\n")
                    f.write(f"- **æè¿°**: {factor['description']}\n")
                    f.write(f"- **å…¬å¼**: `{factor['formula']}`\n")
                    f.write(f"- **ç†ç”±**: {factor['justification']}\n")
                    f.write(f"- **é¢„æœŸè¡¨ç°**: Sharpe={factor['expected_performance']['expected_sharpe']:.2f}\n\n")
        
        print(f"ğŸ“„ æœ€ç»ˆæŠ¥å‘Šå·²ä¿å­˜è‡³: {output_file}")
        return output_file
    
    def test_all_tools(self):
        """æµ‹è¯•æ‰€æœ‰å·¥å…·å‡½æ•°"""
        print("\n" + "="*70)
        print("ğŸ§ª æµ‹è¯•Alpha Research Agentæ‰€æœ‰å·¥å…·")
        print("="*70 + "\n")
        
        # å¯»æ‰¾æµ‹è¯•æ–‡ä»¶
        test_dir = "qlib_data/etf_backup"
        csv_file = None
        
        if os.path.exists(test_dir):
            for fname in os.listdir(test_dir):
                if fname.endswith(".csv"):
                    csv_file = os.path.join(test_dir, fname)
                    break
        
        if not csv_file:
            print("âŒ æœªæ‰¾åˆ°æµ‹è¯•CSVæ–‡ä»¶")
            return
        
        print(f"ğŸ“ ä½¿ç”¨æµ‹è¯•æ–‡ä»¶: {csv_file}\n")
        
        # è¿è¡Œå®Œæ•´å·¥ä½œæµ
        result = self.run_complete_workflow(
            csv_file,
            user_input="è¯·è¿›è¡Œå…¨é¢çš„æŠ€æœ¯åˆ†æå’Œå› å­å»ºè®®"
        )
        
        print("\n" + "="*70)
        print("ğŸ“Š å·¥ä½œæµæ‰§è¡Œç»“æœ:")
        print("="*70)
        print(result)
        print("\n" + "="*70)
        print("âœ… æµ‹è¯•å®Œæˆ!")
        print("="*70 + "\n")
        
        # æ‰“å°æ‰§è¡Œæ—¥å¿—
        self.print_execution_log(detailed=False)
        
        # ä¿å­˜æœ€ç»ˆæŠ¥å‘Š
        report_file = self.save_final_report()
        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Š: {report_file}\n")


# ============================================================================
# ä¸»ç¨‹åºå…¥å£
# ============================================================================

def main():
    """ä¸»ç¨‹åº"""
    # åˆ›å»ºagentå®ä¾‹
    agent = AlphaResearchAgent()
    
    # è¿è¡Œæµ‹è¯•
    agent.test_all_tools()


if __name__ == "__main__":
    main()
